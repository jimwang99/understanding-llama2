@startuml "model"

title "Llama model architecture accoding to facebookresearch/llama/llama/model.py and karpathy/llama2.c/model.py"

skinparam CloudBorderColor White

skinparam component {
    ArrowColor Red
}

header
batch = B
seq_len = L
dim = D
n_heads = Nh
n_kv_heads = Nkv
hidden_dim = Dh
n_layer = Nl
vocab_size = V
end header

footer
Dh = 4 * 2 * D // 3 then round-up to multiple_of
end footer


' Input token
[Input tokens\nTi (B, L)] as Ti

frame Transformer {
    ' Embeddings
    interface "embedding(V, D)" as embedding
    [Input Embeddings\nXi (B, L, D)] as Xi

    Ti --> embedding
    embedding --> Xi

    frame TransformerLayer {
        ' RMS Norm
        () "Attention Norm\nRMSNorm(D)" as attention_norm
        [Attention Input\nXi' (B, L, D)] as Xi1

        Xi --> attention_norm
        attention_norm --> Xi1

        frame "Multi-Head Attention" {
            frame Projection {
                ' Linear projection
                () "linear(D, D)" as projection_q
                () "linear(D, D*Nkv/Nh)" as projection_k
                () "linear(D, D*Nkv/Nh)" as projection_v
                [Q (B, L, D)] as Q
                [K (B, L, D*Nkv/Nh)] as K
                [V (B, L, D*Nkv/Nh)] as V

                Xi1 --> projection_q
                Xi1 --> projection_k
                Xi1 --> projection_v

                projection_q --> Q
                projection_k --> K
                projection_v --> V

                ' Reshape
                [Q' (B, L, Nh, D/Nh)] as Q1
                [K' (B, L, Nkv, D/Nh)] as K1
                [V' (B, L, Nkv, D/Nh)] as V1

                Q --> Q1 : view
                K --> K1 : view
                V --> V1 : view
            }
            ' End of frame "Projection"

            frame "RoPE (Rotary Positional Embedding)" {
                ' Reshape and unbind XQ and XK to match complex representation
                [Q' (B, L, Nh, D/Nh/2, 2)] as Q1_
                [K' (B, L, Nkv, D/Nh/2, 2)] as K1_
                Q1 --> Q1_ : reshape
                K1 --> K1_ : reshape

                () "To complex\nrepresentation\nx.unbind(2)" as complex_q
                () "To complex\nrepresentation\nx.unbind(2)" as complex_k
                [Qc = Qr + Qi*i\nQr (B, L, Nh, D/Nh/2)\nQi (B, L, Nh, D/Nh/2)] as Qc
                [Kc = Kr + Ki*i\nKr (B, L, Nkv, D/Nh/2)\nKi (B, L, Nkv, D/Nh/2)] as Kc

                Q1_ --> complex_q
                K1_ --> complex_k
                complex_q --> Qc
                complex_k --> Kc

                ' RoPE precomputed sin/cos freqs
                database "Cos(mθ)\n(1, L, 1, D/Nh/2)" as Cos
                database "Sin(mθ)\n(1, L, 1, D/Nh/2)" as Sin

                ' Apply rotation on complex
                () "Broadcast dim=[0,2]\nQr'=Qr*Cos-Qi*Sin\nQi'=Qr*Sin+Qi*Cos" as rotate_q
                () "Broadcast dim=[0,2]\nKr'=Kr*Cos-Ki*Sin\nKi'=Kr*Sin+Ki*Cos" as rotate_k

                [Qr' (B, L, Nh, D/Nh/2)\nQi' (B, L, Nh, D/Nh/2)] as Qc1
                [Kr' (B, L, Nkv, D/Nh/2)\nKi' (B, L, Nkv, D/Nh/2)] as Kc1

                Qc --> rotate_q
                Cos --> rotate_q
                Sin --> rotate_q
                Kc --> rotate_k
                Cos --> rotate_k
                Sin --> rotate_k
                rotate_q --> Qc1
                rotate_k --> Kc1


                ' Stack and flatten (like shuffle cards)
                () "stack([Qr', Qi'], dim=-1).flatten(3)" as shuffle_q
                () "stack([Kr', Ki'], dim=-1).flatten(3)" as shuffle_k
                [Q'' (B, L, Nh, D/Nh)] as Q2
                [K'' (B, L, Nkv, D/Nh)] as K2

                Qc1 --> shuffle_q
                shuffle_q --> Q2
                Kc1 --> shuffle_k
                shuffle_k --> K2
            }
            ' End of frame RoPE

            frame "Grouped Multi-Query Attention" {

                frame "Expand KV to match the size of Q" {
                    ' Expand K and V for GQA (grouped multi-query attention)
                    () "repeat_interleave\n(K'', repeats=Nh/Nkv, dim=2)" as repeat_k
                    () "repeat_interleave\n(V', repeats=Nh/Nkv, dim=2)" as repeat_v
                    [Kx (B, L, Nh, D/Nh)] as Kx
                    [Vx (B, L, Nh, D/Nh)] as Vx

                    K2 --> repeat_k
                    repeat_k --> Kx
                    V1 --> repeat_v
                    repeat_v --> Vx
                }

                cloud {
                    ' Transpose
                    () "transpose(1, 2)" as trans_q
                    () "transpose(1, 2)" as trans_k
                    () "transpose(1, 2)" as trans_v

                    [Qt (B, Nh, L, D/Nh)] as Qt
                    [Kt (B, Nh, L, D/Nh)] as Kt
                    [Vt (B, Nh, L, D/Nh)] as Vt

                    Q2 --> trans_q
                    Kx --> trans_k
                    Vx --> trans_v
                    trans_q --> Qt
                    trans_k --> Kt
                    trans_v --> Vt
                }

                frame "Scaled Dot Product Attention" {
                    ' transpose
                    [Kt' (B, Nh, D/Nh, L)] as Kt1
                    () "transpose(2, 3)" as trans_kt

                    Kt --> trans_kt
                    trans_kt --> Kt1

                    ' matmul
                    () "matmul(Q', Kt') / sqrt(D/Nh)" as matmul_q_k
                    [Attention\nA (B, Nh, L, L)] as A

                    Qt --> matmul_q_k
                    Kt1 --> matmul_q_k
                    matmul_q_k --> A

                    ' mask
                    database "Const mask matrix\nM (B, Nh, L, L)" as M
                    () "Add(A, M)" as add_mask
                    [Am (B, Nh, L, L)] as Am

                    A --> add_mask
                    M --> add_mask
                    add_mask --> Am

                    ' softmax
                    () "softmax(Am, dim=-1)" as softmax
                    [As (B, Nh, L, L)] as As

                    Am --> softmax
                    softmax --> As

                    ' matmul
                    () "matmul(As, Vt)" as matmul_a_v
                    [Self-Attention\nSA(B, Nh, L, D/Nh)] as SA

                    Vt --> matmul_a_v
                    As --> matmul_a_v
                    matmul_a_v --> SA
                }
                ' End of frame "Scaled Dot Product Attention"

                ' concat heads
                () "tranpose(1, 2).contiguous()" as concat_sa
                [Self-Attention concatenated\nSAc (B, L, Nh, D/Nh)] as SAc
                [Self-Attention concatenated\nSAc' (B, L, D)] as SAc1

                SA --> concat_sa
                concat_sa --> SAc
                SAc --> SAc1 : view(B, L, -1)

                ' Linear
                () "linear(D, D)" as projection_sac
                [Self-Attention projection\nSAp (B, L, D)] as SAp

                SAc1 --> projection_sac
                projection_sac --> SAp
            }
            ' End of frame "Grouped Multi-Query Attention"
        }

        ' Residual after attention
        () "Attention Residual Add" as attention_residual_add
        [Attention Residual Output\nHa (B, L, D)] as Ha
        Xi --> attention_residual_add
        SAp --> attention_residual_add

        attention_residual_add --> Ha

        ' RMSNorm
        () "FFN Norm\nRMSNorm(-1)" as ffn_norm
        [FFN input\nHi (B, L, D)] as Hi

        Ha --> ffn_norm
        ffn_norm --> Hi

        frame FeedForward {
            cloud {
                () "Gate Projection\nlinear(D, Dh)" as gate_projection
                () "Up Projection\nlinear(D, Dh)" as up_projection
                [Hg (B, L, Dh)] as Hg
                [Hu (B, L, Dh)] as Hu

                Hi --> gate_projection
                gate_projection --> Hg
                Hi --> up_projection
                up_projection --> Hu
            }

            () "swish()" as swish
            [Hs (B, L, Dh)] as Hs

            Hg --> swish
            swish --> Hs

            () "Elem-Wise Mul()" as elem_wise_mul
            [Hm (B, L, Dh)] as Hm
            Hu --> elem_wise_mul
            Hs --> elem_wise_mul
            elem_wise_mul --> Hm

            () "Down Projection\nlinear(Dh, D)" as down_projection
            [Hd (B, L, D)] as Hd

            Hm --> down_projection
            down_projection --> Hd
        }
        ' End of frame FeedForward

        ' Residual after FFN
        () "FNN Residual Add()" as ffn_residual_add
        [FFN Residual Output\nHa (B, L, D)] as Hf
        Ha --> ffn_residual_add
        Hd --> ffn_residual_add
        ffn_residual_add --> Hf
    }


    ' Loop TransformerLayer
    () "(idx < Nl)?" as layer_loop

    Hf --> layer_loop
    layer_loop --> attention_norm : Yes

    ' RMSNorm
    () "Output Norm\nRMSNorm(D)" as output_norm
    [Output Embeddings\nXo (B, L, D)] as Xo

    layer_loop --> output_norm : No
    output_norm --> Xo
}

frame ModelHead {
    ' Output linear
    () "Output Linear\nlinear(D, V)" as output_linear
    [Output Tokens\nTo (B, L, V)] as To

    Xo --> output_linear
    output_linear --> To
}

note right of embedding
A lookup table with 1 as input size, D as output size.
Input value is a unsigned int in the range of [0, V).
Output value is a vector of size D.
end note

note right of attention_norm
Run "Root Mean Square Normalization" on dimension D
`x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + epsilon)`
- rsqrt means reciprocal of the square-root
- x.mean(-1, keepdim=True) outputs shape (B, L, 1)
end note

note right of projection_v
Vector-by-matrix multiplication
- Vector is the dimension D of Xi'
- Matrix is learned weights, no bias
end note

note right of Q1
Split the last dimension D into multiple heads for Q
The same for KV, but their # of heads are smaller than Q
end note

note right of complex_k
Qr = {Q0, Q2, Q4, ...}
Qi = {Q1, Q3, Q5, ...}

Kr = {K0, K2, K4, ...}
Ki = {K1, K3, K5, ...}
end note

note right of Sin
Constants shared by all layers
- m=[0,L)
  - position of token in the sequence
- θ=10000.pow(-2i/d)
  - i=[0,d/2)
  - d=D/Nh
end note

note right of rotate_k
Use memory space to trade for computational complexity

1. Broadcast constant matrix on dimension 0 and 2
2. Elem-wise 4D tensor multiplication

Foundamentially it is
1. Vector-by-matrix element wise multiplication
vector = Cos|Sin(1, ?, 1, D/Nh/2)
matrix = Qx(?, ?, Nh, D/Nh/2)
2. On top of 1, loop through L for both tensors
3. On top of 2, loop through B for Qx and keep Cos|Sin the same
end note

note right of shuffle_k
Q = {Qr0, Qi0, Qr1, Qi1, Qr2, Qi2, ...}
K = {Kr0, Ki0, Kr1, Ki1, Kr2, Ki2, ...}
end note

note bottom of repeat_v
Duplicate the last dimension by Nh/Nkv times
and fold it into 2nd to last dimension

Again, use memory space to trade for computational complexity
end note

note right of Qt
Move the Nh (# of head) dimension to batch domain,
so that scaled-dot-product is done
on L (sequence) and D (embedding) dimensions
end note

note right of M
Constants that are used to mask attentions
from current tokens to future tokens,
so that those attention value's softmax is 0,
by adding -∞ (negative infinity)
end note

note right of softmax
Calculate along the last dimension
<latex>
y_i = e^{x_i} / \sum_i{e^{x_i}}
</latex>
end note

note right of matmul_a_v
NOTE: V needs NO transpose
end note

note right of SAc1
Back to (batch, sequence, embedding)
end note

note right of Hu
Dn (hidden dimension) is ~2.5x of D
end note

note right of output_linear
From embedding to vocabulary probability. Each number
in the last domain represents the probability of corresponding token
end note

@enduml
