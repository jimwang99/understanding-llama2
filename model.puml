@startuml "model"

title "Llama model architecture accoding to facebookresearch/llama/llama/model.py and karpathy/llama2.c/model.py"

skinparam CloudBorderColor White
skinparam interface {
    BackgroundColor<<memops>> Lightblue
    BackgroundColor<<compute>> Pink
    BackgroundColor<<module>> Red
}
skinparam component {
    ArrowColor Red
}


header
batch = B
seq_len = L
max_seq_len = Lm
dim = D
n_heads = Nh
n_kv_heads = Nkv
hidden_dim = Dh
n_layer = Nl
vocab_size = V
end header

footer
Dh = 4 * 2 * D // 3 then round-up to multiple_of
end footer


' Input token
[Input tokens\nTi (B, L)] as Ti

frame Transformer {
    ' Embeddings
    () "embedding(V, D)" as embedding <<memops>>
    [Input Embeddings\nXi (B, L, D)] as Xi

    Ti --> embedding
    embedding --> Xi

    frame TransformerLayer {
        ' RMS Norm
        () "Attention Norm\nRMSNorm(D)" as attention_norm <<module>>
        [Attention Input\nXi' (B, L, D)] as Xi1

        Xi --> attention_norm
        attention_norm --> Xi1

        frame "Multi-Head Attention" {
            frame Projection {
                ' Linear projection
                () "Projection Q\nlinear(D, D)" as projection_q <<module>>
                () "Projection K\nlinear(D, D*Nkv/Nh)" as projection_k <<module>>
                () "Projection V\nlinear(D, D*Nkv/Nh)" as projection_v <<module>>
                [Q (B, L, D)] as Q
                [K (B, L, D*Nkv/Nh)] as K
                [V (B, L, D*Nkv/Nh)] as V

                Xi1 --> projection_q
                Xi1 --> projection_k
                Xi1 --> projection_v

                projection_q --> Q
                projection_k --> K
                projection_v --> V

                ' Reshape
                [Q' (B, L, Nh, D/Nh)] as Q1
                [K' (B, L, Nkv, D/Nh)] as K1
                [V' (B, L, Nkv, D/Nh)] as V1

                Q --> Q1 : view
                K --> K1 : view
                V --> V1 : view
            }
            ' End of frame "Projection"

            frame "RoPE (Rotary Positional Embedding)" {
                ' Reshape and unbind XQ and XK to match complex representation
                [Q' (B, L, Nh, D/Nh/2, 2)] as _Q1
                [K' (B, L, Nkv, D/Nh/2, 2)] as _K1
                Q1 --> _Q1 : reshape
                K1 --> _K1 : reshape

                () "To complex\nrepresentation\nx.unbind(-1)" as complex_q <<memops>>
                () "To complex\nrepresentation\nx.unbind(-1)" as complex_k <<memops>>
                [Qc = Qr + Qi*i\nQr (B, L, Nh, D/Nh/2)\nQi (B, L, Nh, D/Nh/2)] as Qc
                [Kc = Kr + Ki*i\nKr (B, L, Nkv, D/Nh/2)\nKi (B, L, Nkv, D/Nh/2)] as Kc

                _Q1 --> complex_q
                _K1 --> complex_k
                complex_q --> Qc
                complex_k --> Kc

                ' RoPE precomputed sin/cos freqs
                database "Cos(mθ)\n(1, L, 1, D/Nh/2)" as Cos
                database "Sin(mθ)\n(1, L, 1, D/Nh/2)" as Sin

                ' Apply rotation on complex
                () "Broadcast dim=[0,2]\nQr'=Qr*Cos-Qi*Sin\nQi'=Qr*Sin+Qi*Cos" as rotate_q <<compute>>
                () "Broadcast dim=[0,2]\nKr'=Kr*Cos-Ki*Sin\nKi'=Kr*Sin+Ki*Cos" as rotate_k <<compute>>

                [Qr' (B, L, Nh, D/Nh/2)\nQi' (B, L, Nh, D/Nh/2)] as Qc1
                [Kr' (B, L, Nkv, D/Nh/2)\nKi' (B, L, Nkv, D/Nh/2)] as Kc1

                Qc --> rotate_q
                Cos --> rotate_q
                Sin --> rotate_q
                Kc --> rotate_k
                Cos --> rotate_k
                Sin --> rotate_k
                rotate_q --> Qc1
                rotate_k --> Kc1


                ' Stack and flatten (like shuffle cards)
                () "stack([Qr', Qi'], dim=-1).flatten(3)" as shuffle_q <<memops>>
                () "stack([Kr', Ki'], dim=-1).flatten(3)" as shuffle_k <<memops>>
                [Q'' (B, L, Nh, D/Nh)] as Q2
                [K'' (B, L, Nkv, D/Nh)] as K2

                Qc1 --> shuffle_q
                shuffle_q --> Q2
                Kc1 --> shuffle_k
                shuffle_k --> K2
            }
            ' End of frame RoPE

            frame "Grouped Multi-Query Attention" {

                frame "Repeat KV to match the size of Q" {
                    ' Repeat K and V for GQA (grouped multi-query attention)
                    () "repeat_interleave\n(K'', repeats=Nh/Nkv, dim=2)" as repeat_k <<memops>>
                    () "repeat_interleave\n(V', repeats=Nh/Nkv, dim=2)" as repeat_v <<memops>>
                    [Kx (B, L, Nh, D/Nh)] as Kx
                    [Vx (B, L, Nh, D/Nh)] as Vx

                    K2 --> repeat_k
                    repeat_k --> Kx
                    V1 --> repeat_v
                    repeat_v --> Vx
                }

                cloud {
                    ' Transpose
                    () "transpose(1, 2)" as trans_q <<memops>>
                    () "transpose(1, 2)" as trans_k <<memops>>
                    () "transpose(1, 2)" as trans_v <<memops>>

                    [Qt (B, Nh, L, D/Nh)] as Qt
                    [Kt (B, Nh, L, D/Nh)] as Kt
                    [Vt (B, Nh, L, D/Nh)] as Vt

                    Q2 --> trans_q
                    Kx --> trans_k
                    Vx --> trans_v
                    trans_q --> Qt
                    trans_k --> Kt
                    trans_v --> Vt
                }

                frame "Scaled Dot Product Attention" {
                    ' transpose
                    [Kt' (B, Nh, D/Nh, L)] as Kt1
                    () "transpose(2, 3)" as trans_kt <<memops>>

                    Kt --> trans_kt
                    trans_kt --> Kt1

                    ' matmul
                    () "matmul(Qt, Kt') / sqrt(D/Nh)" as matmul_q_k <<compute>>
                    [Attention\nA (B, Nh, L, L)] as A

                    Qt --> matmul_q_k
                    Kt1 --> matmul_q_k
                    matmul_q_k --> A

                    ' mask
                    database "Const mask matrix\nM (B, Nh, L, L)" as M
                    () "add(A, M)" as add_mask <<compute>>
                    [Am (B, Nh, L, L)] as Am

                    A --> add_mask
                    M --> add_mask
                    add_mask --> Am

                    ' softmax
                    () "softmax(Am, dim=-1)" as softmax <<compute>>
                    [As (B, Nh, L, L)] as As

                    Am --> softmax
                    softmax --> As

                    ' matmul
                    () "matmul(As, Vt)" as matmul_a_v <<compute>>
                    [Self-Attention\nSA(B, Nh, L, D/Nh)] as SA

                    Vt --> matmul_a_v
                    As --> matmul_a_v
                    matmul_a_v --> SA
                }
                ' End of frame "Scaled Dot Product Attention"

                ' concat heads
                () "tranpose(1, 2).contiguous()" as concat_sa <<memops>>
                [Self-Attention concatenated\nSAc (B, L, Nh, D/Nh)] as SAc
                [Self-Attention concatenated\nSAc' (B, L, D)] as SAc1

                SA --> concat_sa
                concat_sa --> SAc
                SAc --> SAc1 : view(B, L, -1)

                ' Linear
                () "Projection A\nlinear(D, D)" as projection_sac <<module>>
                [Self-Attention projection\nSAp (B, L, D)] as SAp

                SAc1 --> projection_sac
                projection_sac --> SAp
            }
            ' End of frame "Grouped Multi-Query Attention"
        }

        ' Residual after attention
        () "Attention Residual Add" as attention_residual_add <<compute>>
        [Attention Residual Output\nHa (B, L, D)] as Ha
        Xi --> attention_residual_add
        SAp --> attention_residual_add

        attention_residual_add --> Ha

        ' RMSNorm
        () "FFN Norm\nRMSNorm(D)" as ffn_norm <<module>>
        [FFN input\nHi (B, L, D)] as Hi

        Ha --> ffn_norm
        ffn_norm --> Hi

        frame FeedForward {
            cloud {
                () "Gate Projection\nlinear(D, Dh)" as gate_projection <<module>>
                () "Up Projection\nlinear(D, Dh)" as up_projection <<module>>
                [Hg (B, L, Dh)] as Hg
                [Hu (B, L, Dh)] as Hu

                Hi --> gate_projection
                gate_projection --> Hg
                Hi --> up_projection
                up_projection --> Hu
            }

            () "swish()" as swish <<compute>>
            [Hs (B, L, Dh)] as Hs

            Hg --> swish
            swish --> Hs

            () "Elem-Wise Mul()" as elem_wise_mul <<compute>>
            [Hm (B, L, Dh)] as Hm
            Hu --> elem_wise_mul
            Hs --> elem_wise_mul
            elem_wise_mul --> Hm

            () "Down Projection\nlinear(Dh, D)" as down_projection <<module>>
            [Hd (B, L, D)] as Hd

            Hm --> down_projection
            down_projection --> Hd
        }
        ' End of frame FeedForward

        ' Residual after FFN
        () "FNN Residual Add(Hd, Ha)" as ffn_residual_add <<compute>>
        [FFN Residual Output\nHf (B, L, D)] as Hf
        Ha --> ffn_residual_add
        Hd --> ffn_residual_add
        ffn_residual_add --> Hf
    }


    ' Loop TransformerLayer
    () "Looping through the layers" as layer_loop
    [Input Embeddings\nXi (B, L, D)] as _Xi

    Hf --> layer_loop
    layer_loop -> _Xi : Loop is not finished\n(to the next layer's input)

}

' RMSNorm
() "Output Norm\nRMSNorm(D)" as output_norm <<module>>
[Output Embeddings\nXo (B, L, D)] as Xo

layer_loop --> output_norm : Loop is finished
output_norm --> Xo

' Output linear
() "Output Linear\nlinear(D, V)" as output_linear <<module>>
[Output Logits\nLo (B, L|1, V)] as Lo

Xo --> output_linear
output_linear --> Lo

note right of embedding
A lookup table with 1 as input size, D as output size.
Input value is a unsigned int in the range of [0, V).
Output value is a vector of size D.
end note

note right of attention_norm
Run "Root Mean Square Normalization" on dimension D
`x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + epsilon)`
- rsqrt means reciprocal of the square-root
- x.mean(-1, keepdim=True) outputs shape (B, L, 1)
end note

note right of projection_v
Vector-by-matrix multiplication
- Vector is the dimension D of Xi'
- Matrix is learned weights, no bias
end note

note right of Q1
Split the last dimension D into multiple heads for Q
The same for KV, but their # of heads are smaller than Q
end note

note right of complex_k
Qr = {Q0, Q2, Q4, ...}
Qi = {Q1, Q3, Q5, ...}

Kr = {K0, K2, K4, ...}
Ki = {K1, K3, K5, ...}
end note

note right of Sin
Constants shared by all layers
- m=[0,L)
  - position of token in the sequence
- θ=10000.pow(-2i/d)
  - i=[0,d/2)
  - d=D/Nh
end note

note right of rotate_k
Use memory space to trade for computational complexity

1. Broadcast constant matrix on dimension 0 and 2
2. Elem-wise 4D tensor multiplication

Foundamentially it is
1. Vector-by-matrix element wise multiplication
vector = Cos|Sin(1, ?, 1, D/Nh/2)
matrix = Qx(?, ?, Nh, D/Nh/2)
2. On top of 1, loop through L for both tensors
3. On top of 2, loop through B for Qx and keep Cos|Sin the same
end note

note right of shuffle_k
Q = {Qr0, Qi0, Qr1, Qi1, Qr2, Qi2, ...}
K = {Kr0, Ki0, Kr1, Ki1, Kr2, Ki2, ...}
end note

note bottom of repeat_v
Duplicate the last dimension by Nh/Nkv times
and fold it into 2nd to last dimension

Again, use memory space to trade for computational complexity
end note

note right of Qt
Move the Nh (# of head) dimension to batch domain,
so that scaled-dot-product is done
on L (sequence) and D (embedding) dimensions
end note

note right of M
Constants that are used to mask attentions
from current tokens to future tokens,
so that those attention value's softmax is 0,
by adding -∞ (negative infinity)
end note

note right of softmax
Calculate along the last dimension
<latex>
y_i = e^{x_i} / \sum_i{e^{x_i}}
</latex>
end note

note right of matmul_a_v
NOTE: V needs NO transpose
end note

note right of SAc1
Back to (batch, sequence, embedding)
end note

note right of Hu
Dn (hidden dimension) is ~2.5x of D
end note

note right of output_linear
From embedding to vocabulary probability. Each number
in the last domain represents the probability of corresponding token
end note

note right of Lo
Output of the model is a list of logits (last dimension, V) that
represent the values associated with every tokens in the vocaburary.
Then we use sampling algorithm to pick the next token.

The L dimension (second to last) actually means the model predicted
next token of every position of the input sequence, and it's used in
training mode. If in inference mode, we only need the last one, which
is the future (unknown) token.
end note


note right of _Xi
Goes back to the input of Transformer Layer.
> Drawing the arrow will mess up PlantUML's engine
> and make this diagram unreadable, therefore it's not shown here.
end note


@enduml
